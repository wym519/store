{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZbEWTJbzQ8B"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "MAX_TITLE_LENGTH=30\n",
    "MAX_NEWS=50\n",
    "MAX_BODY_LENGTH=200\n",
    "npratio=4\n",
    "\n",
    "with open('train.tsv') as f:\n",
    "    trainuser=f.readlines()\n",
    "\n",
    "with open('test.tsv') as f:\n",
    "    testuser=f.readlines()\n",
    "\n",
    "with open('docs.tsv') as f:\n",
    "    data=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZbEWTJbzQ8B"
   },
   "outputs": [],
   "source": [
    "news={} \n",
    "for i in data:\n",
    "    line=i.split('\\t')\n",
    "    body_tokens=word_tokenize(line[4].lower())\n",
    "    news[line[0]]=[line[1],line[2],word_tokenize(line[3].lower()),body_tokens, max(len(body_tokens),1)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZbEWTJbzQ8B"
   },
   "outputs": [],
   "source": [
    "def newsample(negatives,ratio):\n",
    "    if ratio >len(negatives):\n",
    "        return random.sample(negatives*(ratio//len(negatives)+1),ratio)\n",
    "    else:\n",
    "        return random.sample(negatives,ratio)\n",
    "\n",
    "train_pn=[]   \n",
    "train_p=[]\n",
    "train_label=[]\n",
    "train_label_speed=[]\n",
    "train_user_his=[]\n",
    "train_user_his_satis=[] \n",
    "for i in trainuser:\n",
    "  \n",
    "    line=i.replace('\\n','').split('\\t')\n",
    "\n",
    "    clickids=[newsindex[x] for x in line[3].split()][-MAX_NEWS:]\n",
    "    clicknewslen=[news[x][4] for x in line[3].split()][-MAX_NEWS:]\n",
    "    dwelltime=[int(x) for x in line[4].split()][-MAX_NEWS:]\n",
    "    readspeed=[clicknewslen[_]/(dwelltime[_]+1) for _ in range(len(dwelltime))]\n",
    "    avgreadspeed=np.mean(readspeed)\n",
    "    readspeednorm=[min(max(np.log2(avgreadspeed/x)+6,0),13) for x in readspeed]\n",
    "    \n",
    "    pdoc=[newsindex[x] for x in line[7].split()]\n",
    "    ndoc=[newsindex[x] for x in line[8].split()]\n",
    "    dwelltimewordvec=[int(x) for x in line[11].split()]\n",
    "    wordvecnewslen=[news[x][4] for x in line[7].split()] \n",
    "    readspeedwordvec=[wordvecnewslen[_]/(dwelltimewordvec[_]+1) for _ in range(len(dwelltimewordvec))]\n",
    "    readspeedwordvecnorm=[min(max(np.log2(avgreadspeed/x)+6,0),13)/13. for x in readspeedwordvec]\n",
    "    \n",
    "    for pdocindex in range(len(pdoc)):\n",
    "        pdocid=pdoc[pdocindex]\n",
    "        npdocindex=newsample(ndoc,npratio)\n",
    "        npdocindex.append(pdocid)\n",
    "        label=[0]*npratio+[1] \n",
    "        train_pn.append(npdocindex)\n",
    "        train_p.append(pdocid)\n",
    "        train_label.append(label)\n",
    "        train_user_his.append(clickids+[0]*(MAX_NEWS-len(clickids)))\n",
    "        train_user_his_satis.append(readspeednorm+[0]*(MAX_NEWS-len(clickids)))\n",
    "        train_label_speed.append(readspeedwordvecnorm[pdocindex])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZbEWTJbzQ8B"
   },
   "outputs": [],
   "source": [
    "test_pn=[]    \n",
    "test_label=[]\n",
    "test_user_his_satis=[]\n",
    "test_impression_index=[]\n",
    "for i in testuser:\n",
    "    line=i.replace('\\n','').split('\\t')\n",
    "\n",
    "    clickids=[newsindex[x] for x in line[3].split()][-MAX_NEWS:]\n",
    "    dwelltime=[int(x) for x in line[4].split()][-MAX_NEWS:]\n",
    "    clicknewslen=[news[x][4] for x in line[3].split()][-MAX_NEWS:]\n",
    "    readspeed=[clicknewslen[_]/(dwelltime[_]+1) for _ in range(len(dwelltime))]\n",
    "    avgreadspeed=np.mean(readspeed)\n",
    "    readspeednorm=[min(max(np.log2(avgreadspeed/x)+6,0),13) for x in readspeed]\n",
    "\n",
    "    \n",
    "    pdoc=[newsindex[x] for x in line[7].split()][:MAX_NEWS]\n",
    "    ndoc=[newsindex[x] for x in line[8].split()][:300]\n",
    "    impression_start_end=[]\n",
    "    impression_start_end.append(len(test_pn))\n",
    "\n",
    "    for mp in pdoc:\n",
    "\n",
    "        test_pn.append(mp)\n",
    "        test_label.append(1)\n",
    "        test_user_his.append(clickids+[0]*(MAX_NEWS-len(clickids)))\n",
    "        test_user_his_satis.append(readspeednorm+[0]*(MAX_NEWS-len(clickids)))\n",
    "        \n",
    "    for mp in ndoc:\n",
    "\n",
    "        test_pn.append(mp)\n",
    "        test_label.append(0)\n",
    "        test_user_his.append(clickids+[0]*(MAX_NEWS-len(clickids)))\n",
    "        test_user_his_satis.append(readspeednorm+[0]*(MAX_NEWS-len(clickids)))\n",
    "    impression_start_end.append(len(test_pn))\n",
    "    test_impression_index.append(impression_start_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZbEWTJbzQ8B"
   },
   "outputs": [],
   "source": [
    "word_dict_raw={'PADDING':[0,999999]}\n",
    "\n",
    "for i in news:\n",
    "    for j in news[i][2]:\n",
    "        if j in word_dict_raw:\n",
    "            word_dict_raw[j][1]+=1\n",
    "        else:\n",
    "            word_dict_raw[j]=[len(word_dict_raw),1]\n",
    "            \n",
    "for i in news:\n",
    "    for j in news[i][3]:\n",
    "        if j in word_dict_raw:\n",
    "            word_dict_raw[j][1]+=1\n",
    "        else:\n",
    "            word_dict_raw[j]=[len(word_dict_raw),1]\n",
    "\n",
    "\n",
    "word_dict={}\n",
    "for i in word_dict_raw:\n",
    "    if word_dict_raw[i][1]>=3:\n",
    "        word_dict[i]=[len(word_dict),word_dict_raw[i][1]]\n",
    "print(len(word_dict),len(word_dict_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZbEWTJbzQ8B"
   },
   "outputs": [],
   "source": [
    "embdict={} \n",
    "import pickle\n",
    "with open('/data/wuch/glove.840B.300d.txt','rb')as f: \n",
    "    while True:\n",
    "        line=f.readline()\n",
    "        if len(j)==0:\n",
    "            break\n",
    "        line=line.split() \n",
    "        word=line[0].decode() \n",
    "        if len(word) != 0:\n",
    "            vec=[float(x) for x in line[1:]]\n",
    "            if word in word_dict:\n",
    "                embdict[word]=vec \n",
    "\n",
    "\n",
    "from numpy.linalg import cholesky\n",
    "emb_table=[0]*len(word_dict) \n",
    "wordvec=[]\n",
    "for i in embdict.keys():\n",
    "    emb_table[word_dict[i][0]]=np.array(embdict[i],dtype='float32')\n",
    "    wordvec.append(emb_table[word_dict[i][0]])\n",
    "wordvec=np.array(wordvec,dtype='float32')\n",
    "\n",
    "mu=np.mean(wordvec, axis=0)\n",
    "Sigma=np.cov(wordvec.T)\n",
    "\n",
    "norm=np.random.multivariate_normal(mu, Sigma, 1) \n",
    "\n",
    "for i in range(len(emb_table)):\n",
    "    if type(emb_table[i])==int:\n",
    "        emb_table[i]=np.reshape(norm, 300)\n",
    "        \n",
    "emb_table[0]=np.zeros(300,dtype='float32')\n",
    "emb_table=np.array(emb_table,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZbEWTJbzQ8B"
   },
   "outputs": [],
   "source": [
    "news_words=[[0]*MAX_TITLE_LENGTH]\n",
    "\n",
    "for i in news:\n",
    "    line=[]\n",
    "    for word in news[i][2]:\n",
    "        if word in word_dict:\n",
    "            line.append(word_dict[j][0])\n",
    "    line=line[:MAX_TITLE_LENGTH]\n",
    "    news_words.append(line+[0]*(MAX_TITLE_LENGTH-len(line)))\n",
    "\n",
    "news_body=[[0]*MAX_BODY_LENGTH]\n",
    "\n",
    "for i in news:\n",
    "    line=[]\n",
    "    for word in news[i][3]:\n",
    "\n",
    "        if word in word_dict:\n",
    "            line.append(word_dict[j][0])\n",
    "    line=line[:MAX_BODY_LENGTH]\n",
    "    \n",
    "    \n",
    "    news_body.append(line+[0]*(MAX_BODY_LENGTH-len(line)))\n",
    "\n",
    "\n",
    "news_words=np.array(news_words,dtype='int32') \n",
    "news_body=np.array(news_body,dtype='int32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZbEWTJbzQ8B"
   },
   "outputs": [],
   "source": [
    "train_p =np.array(train_p ,dtype='int32')\n",
    "train_pn=np.array(train_pn,dtype='int32')\n",
    "train_label=np.array(train_label,dtype='int32')\n",
    "train_user_his=np.array(train_user_his,dtype='int32')\n",
    "train_user_his_satis=np.array(train_user_his_satis,dtype='int32')\n",
    "train_label_speed=np.array(train_label_speed,dtype='float32')\n",
    "\n",
    "test_pn=np.array(test_pn,dtype='int32')\n",
    "test_label=np.array(test_label,dtype='int32')\n",
    "test_user_his=np.array(test_user_his,dtype='int32')\n",
    "test_user_his_satis=np.array(test_user_his_satis,dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZbEWTJbzQ8B"
   },
   "outputs": [],
   "source": [
    "def generate_batch_data(batch_size):\n",
    "    idx = np.arange(len(label))\n",
    "    np.random.shuffle(idx)\n",
    "    y=label\n",
    "    batches = [idx[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            item_title = news_words[train_pn[i]]\n",
    "            item_title_split=[item_title[:,k,:] for k in range(item_title.shape[1])]\n",
    "\n",
    "            user_title=news_words[user_his[i]]\n",
    "            user_satis=train_user_his_satis[i]\n",
    "            item_body = news_body[train_pn[i]]\n",
    "            item_body_split=[item_body[:,k,:] for k in range(item_body.shape[1])]\n",
    "            \n",
    "            user_body=news_body[user_his[i]]\n",
    "            item_pos_title = news_words[train_p[i]]\n",
    "            item_pos_body = news_body[train_p[i]] \n",
    "            \n",
    "            yield (item_title_split+[user_title]+item_body_split+[user_body,user_satis,item_pos_title,item_pos_body], [train_label[i],train_label_speed[i]])\n",
    "\n",
    "def generate_batch_data_test(batch_size):\n",
    "    \n",
    "\n",
    "    idx = np.arange(len(test_label))\n",
    "    y=test_label\n",
    "    batches = [idx[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            item_title = news_words[test_pn[i]]\n",
    "            user_title=news_words[test_user_his[i]]\n",
    "            user_satis=test_user_his_satis[i]\n",
    "            item_body = news_body[test_pn[i]]\n",
    "            user_body=news_body[test_user_his[i]] \n",
    "            \n",
    "            yield ([item_title,user_title,item_body,user_body,user_satis], [test_label[i]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZbEWTJbzQ8B"
   },
   "outputs": [],
   "source": [
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gains = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best = dcg_score(y_true, y_true, k)\n",
    "    actual = dcg_score(y_true, y_score, k)\n",
    "    return actual / best\n",
    "\n",
    "\n",
    "def mrr_score(y_true, y_score):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order)\n",
    "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
    "    return np.sum(rr_score) / np.sum(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZbEWTJbzQ8B"
   },
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers \n",
    "from sklearn.metrics import accuracy_score, classification_report,roc_auc_score\n",
    "from keras.optimizers import *\n",
    "import keras\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZbEWTJbzQ8B"
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self, nb_head, size_per_head, **kwargs):\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "        self.output_dim = nb_head*size_per_head\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.WQ = self.add_weight(name='WQ', \n",
    "                                  shape=(input_shape[0][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WK = self.add_weight(name='WK', \n",
    "                                  shape=(input_shape[1][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WV = self.add_weight(name='WV', \n",
    "                                  shape=(input_shape[2][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def Mask(self, inputs, seq_len, mode='mul'):\n",
    "        if seq_len == None:\n",
    "            return inputs\n",
    "        else:\n",
    "            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n",
    "            mask = 1 - K.cumsum(mask, 1)\n",
    "            for _ in range(len(inputs.shape)-2):\n",
    "                mask = K.expand_dims(mask, 2)\n",
    "            if mode == 'mul':\n",
    "                return inputs * mask\n",
    "            if mode == 'add':\n",
    "                return inputs - (1 - mask) * 1e12\n",
    "                \n",
    "    def call(self, x):\n",
    "        if len(x) == 3:\n",
    "            Q_seq,K_seq,V_seq = x\n",
    "            Q_len,V_len = None,None\n",
    "        elif len(x) == 5:\n",
    "            Q_seq,K_seq,V_seq,Q_len,V_len = x\n",
    "        Q_seq = K.dot(Q_seq, self.WQ)\n",
    "        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n",
    "        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n",
    "        K_seq = K.dot(K_seq, self.WK)\n",
    "        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n",
    "        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n",
    "        V_seq = K.dot(V_seq, self.WV)\n",
    "        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n",
    "        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))\n",
    "        A = K.batch_dot(Q_seq, K_seq, axes=[3,3]) / self.size_per_head**0.5\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))\n",
    "        A = self.Mask(A, V_len, 'add')\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))    \n",
    "        A = K.softmax(A)\n",
    "        O_seq = K.batch_dot(A, V_seq, axes=[3,2])\n",
    "        O_seq = K.permute_dimensions(O_seq, (0,2,1,3))\n",
    "        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))\n",
    "        O_seq = self.Mask(O_seq, Q_len, 'mul')\n",
    "        return O_seq\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZbEWTJbzQ8B"
   },
   "outputs": [],
   "source": [
    "results=[]\n",
    "for repeat in range(5):\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    \n",
    "    title_input = Input(shape=(MAX_TITLE_LENGTH,), dtype='int32')\n",
    "    \n",
    "    body_input= Input(shape=(MAX_BODY_LENGTH,), dtype='int32')\n",
    "    \n",
    "    embedding_layer = Embedding(len(word_dict), 300, weights=[emb_table],trainable=True)\n",
    "    \n",
    "    embedded_sequences = embedding_layer(title_input)\n",
    "    drop_emb=Dropout(0.2)(embedded_sequences)\n",
    "    \n",
    "    selfatt = Attention(16,16)([drop_emb,drop_emb,drop_emb]) \n",
    "    drop_selfatt=Dropout(0.2)(selfatt)\n",
    "    \n",
    "    attention = Dense(200,activation='tanh')(drop_selfatt)\n",
    "    attention = Flatten()(Dense(1)(attention))\n",
    "    attention_weight = Activation('softmax')(attention)\n",
    "    title_rep=keras.layers.Dot((1, 1))([drop_selfatt, attention_weight])\n",
    "    \n",
    "    \n",
    "    title_encoder = Model([title_input], title_rep)\n",
    "    \n",
    "    embedded_sequences2 = embedding_layer(body_input)\n",
    "    drop_emb2=Dropout(0.2)(embedded_sequences2)\n",
    "    \n",
    "    selfatt2 = Attention(16,16)([drop_emb2,drop_emb2,drop_emb2]) \n",
    "    drop_selfatt2=Dropout(0.2)(selfatt2)\n",
    "    \n",
    "    attention2 = Dense(200,activation='tanh')(drop_selfatt2)\n",
    "    attention2 = Flatten()(Dense(1)(attention2))\n",
    "    attention_weight2 = Activation('softmax')(attention2)\n",
    "    body_rep=keras.layers.Dot((1, 1))([drop_selfatt2, attention_weight2])\n",
    "\n",
    "    bodyEncodert = Model([body_input], body_rep)\n",
    "    \n",
    "    his_title_input =  Input((MAX_NEWS, MAX_SENT_LENGTH,), dtype='int32') \n",
    "    his_body_input = Input((MAX_NEWS,200,), dtype='int32')\n",
    " \n",
    "    titlebeh=TimeDistributed(title_encoder)(his_title_input)\n",
    "    bodybeh=TimeDistributed(bodyEncodert)(his_body_input)\n",
    "    \n",
    "    attention_title = Dense(200,activation='tanh')(titlebeh)\n",
    "    attention_title = Flatten()(Dense(1)(attention_title))\n",
    "    attention_weight_title = Activation('softmax')(attention_title)\n",
    "    userrep_title=keras.layers.Dot((1, 1))([titlebeh, attention_weight_title])\n",
    "    \n",
    "    attention_body = Dense(200,activation='tanh')(bodybeh)\n",
    "    attention_body = Flatten()(Dense(1)(attention_body))\n",
    "    attention_weight_body = Activation('softmax')(attention_body)\n",
    "    userrep_body=keras.layers.Dot((1, 1))([bodybeh, attention_weight_body])\n",
    "\n",
    "\n",
    "    time_input = Input(shape=(MAX_NEWS,), dtype='int32')\n",
    "    timeembedding_layer = Embedding(100, 50,  trainable=True)(time_input)\n",
    "    attention_satis = Lambda(lambda x:K.sum(x,axis=-1))(multiply([bodybeh, Dense(256)(timeembedding_layer) ]))\n",
    "    attention_weight_satis = Activation('softmax')(attention_satis)\n",
    "    userrep_satis=keras.layers.Dot((1, 1))([bodybeh, attention_weight_satis])\n",
    "    userrep_read=add([userrep_body,userrep_satis])\n",
    "    \n",
    "    uservecs =concatenate([Lambda(lambda x: K.expand_dims(x,axis=1))(vec) for vec in [userrep_title,userrep_read]],axis=1)\n",
    "    attentionvecs= Dense(200,activation='tanh')(uservecs)\n",
    "    attentionvecs = Flatten()(Dense(1)(attentionvecs))\n",
    "    attention_weightvecs = Activation('softmax')(attentionvecs)\n",
    "    userrep=keras.layers.Dot((1, 1))([uservecs, attention_weightvecs])\n",
    "    \n",
    "    cand_title =[Input((MAX_SENT_LENGTH,), dtype='int32') for _ in range(1+npratio)]\n",
    "    cand_body =[Input(( MAX_BODY_LENGTH,), dtype='int32')  for _ in range(1+npratio)]\n",
    "    cand_titlerep=[ title_encoder(cand_title[_]) for _ in range(1+npratio)]\n",
    "    #cand_bodyrep=[ bodyEncodert(cand_body[_]) for _ in range(1+npratio)]\n",
    "    \n",
    "    cand_pos_title = keras.Input((MAX_SENT_LENGTH,))\n",
    "    cand_pos_body = keras.Input((MAX_BODY_LENGTH,)) \n",
    "    cand_one_bodyrep = bodyEncodert([cand_pos_body])\n",
    "    \n",
    "    dense1=Dense(100)\n",
    "    dense2=Dense(100)\n",
    "    predspeed =  keras.layers.dot([dense1(userrep), dense2(cand_one_bodyrep)], axes=-1) \n",
    "    logits = concatenate([keras.layers.dot([userrep, cvec], axes=-1) for cvec in cand_titlerep])\n",
    "    logits = keras.layers.Activation(keras.activations.softmax)(logits)\n",
    "\n",
    "    model = Model(cand_title+[his_title_input]+cand_body+[his_body_input,time_input,cand_pos_title,cand_pos_body], [logits,predspeed ])\n",
    "    model.compile(loss=['categorical_crossentropy','mae'], optimizer=Adam(lr=0.001), metrics=['acc','mae'],loss_weights=[1.,0.4])\n",
    "    \n",
    "    \n",
    "    cand_one_title = keras.Input((MAX_SENT_LENGTH,))\n",
    "    cand_one_body = keras.Input((MAX_BODY_LENGTH,))\n",
    "    cand_one_vec = title_encoder([cand_one_title])\n",
    "            \n",
    "    score = keras.layers.Activation(keras.activations.sigmoid)(dot([userrep, cand_one_vec], axes=-1))\n",
    "    model_test = keras.Model([cand_one_title,his_title_input,cand_one_body,his_body_input,time_input], score)\n",
    "   \n",
    "    for ep in range(2):\n",
    "        traingen=generate_batch_data(30)\n",
    "        model.fit_generator(traingen, epochs=1,steps_per_epoch=len(label)//30)\n",
    "    testgen=generate_batch_data_test(30)\n",
    "    auc=[]\n",
    "    mrr=[]\n",
    "    ndcg5=[]\n",
    "    ndcg10=[]\n",
    "    pred = model_test.predict_generator(testgen, steps=len(test_pn)//30,verbose=1)\n",
    "    \n",
    "    auc=[]\n",
    "    mrr=[]\n",
    "    ndcg5=[]\n",
    "    ndcg10=[]\n",
    "    for m in test_impression_index:\n",
    "        if np.sum(test_label[m[0]:m[1]])!=0 and m[1]<len(pred):\n",
    "            auc.append(roc_auc_score(test_label[m[0]:m[1]],pred[m[0]:m[1],0]))\n",
    "            mrr.append(mrr_score(test_label[m[0]:m[1]],pred[m[0]:m[1],0]))\n",
    "            ndcg5.append(ndcg_score(test_label[m[0]:m[1]],pred[m[0]:m[1],0],k=5))\n",
    "            ndcg10.append(ndcg_score(test_label[m[0]:m[1]],pred[m[0]:m[1],0],k=10))\n",
    "    results.append([np.mean(auc),np.mean(mrr),np.mean(ndcg5),np.mean(ndcg10)])\n",
    "    print(np.mean(auc),np.mean(mrr),np.mean(ndcg5),np.mean(ndcg10))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "time.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
